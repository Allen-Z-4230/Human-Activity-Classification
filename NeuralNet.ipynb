{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.neural_network as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(confusion):\n",
    "    # Evaluate the model based on accuracy, percision, recall, and BCR\n",
    "    acc_list = []\n",
    "    percision_list = []\n",
    "    recall_list = []\n",
    "    BCR_list = []\n",
    "\n",
    "    for i in range(len(confusion)):\n",
    "        # True positives\n",
    "        tp = confusion[i][i]\n",
    "        # False positives\n",
    "        fp = np.sum(confusion[i]) - tp\n",
    "        # False negitives\n",
    "        fn = np.sum(confusion[:,i]) - tp\n",
    "        # True negitives\n",
    "        tn = np.sum(confusion) - tp - fp - fn\n",
    "\n",
    "        # accuracy = (tp + tn) / (tp+ tn + fp + fn)\n",
    "        acc_list.append((tp + tn) / (tp+ tn + fp + fn))\n",
    "        # percision = tp / (tp + fp)\n",
    "        if tp + fp != 0:\n",
    "            percision_list.append(tp / (tp + fp))\n",
    "        else:\n",
    "            percision_list.append(0)\n",
    "        # recall = tp / (tp + fn)\n",
    "        recall_list.append(tp / (tp + fn))\n",
    "        # BCR = (percision + recall) / 2\n",
    "        BCR_list.append((percision_list[-1] + recall_list[-1]) / 2)\n",
    "    \n",
    "    return acc_list, percision_list, recall_list, BCR_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the data into a dataframe to look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X    ...     \\\n",
       "0         -0.923527         -0.934724    ...      \n",
       "1         -0.957686         -0.943068    ...      \n",
       "2         -0.977469         -0.938692    ...      \n",
       "3         -0.989302         -0.938692    ...      \n",
       "4         -0.990441         -0.942469    ...      \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.710304                    -0.112754   \n",
       "1                        -0.861499                     0.053477   \n",
       "2                        -0.760104                    -0.118559   \n",
       "3                        -0.482845                    -0.036788   \n",
       "4                        -0.699205                     0.123320   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.030400                         -0.464761   \n",
       "1                             -0.007435                         -0.732626   \n",
       "2                              0.177899                          0.100699   \n",
       "3                             -0.012892                          0.640011   \n",
       "4                              0.122542                          0.693578   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                             -0.018446             -0.841247   \n",
       "1                              0.703511             -0.844788   \n",
       "2                              0.808529             -0.848933   \n",
       "3                             -0.485366             -0.848649   \n",
       "4                             -0.615971             -0.847865   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  subject  Activity  \n",
       "0              0.179941             -0.058627        1  STANDING  \n",
       "1              0.180289             -0.054317        1  STANDING  \n",
       "2              0.180637             -0.049118        1  STANDING  \n",
       "3              0.181935             -0.047663        1  STANDING  \n",
       "4              0.185151             -0.043892        1  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, let's look at the categories in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
       "       'WALKING_UPSTAIRS'], dtype='<U18')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(list(df_train['Activity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 6 categories in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Two Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try out our network by using just 2 categories. We will do an easy task where we will compare two categories that are vastly differnet like laying and walking. In order to do that, we will filter the data to find only data that is of someone laying or walking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_list = [\"WALKING\", \"LAYING\"]\n",
    "\n",
    "train = df_train[df_train.Activity.isin(activity_list)].values\n",
    "test = df_test[df_test.Activity.isin(activity_list)].values\n",
    "\n",
    "# Create the training and testing set and shuffle the data\n",
    "train_X = train[:,:-2]\n",
    "train_Y = train[:,-1]\n",
    "train_X, train_Y = shuffle(train_X, train_Y)\n",
    "\n",
    "test_X = test[:,:-2]\n",
    "test_Y = test[:,-1]\n",
    "test_X, test_Y = shuffle(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a training and a testing set, we will train a simple neural net with only two layer with 100 neruons in the first layer and 50 in the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.MLPClassifier(hidden_layer_sizes=(100,50)).fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010978582085550533"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing out the training loss of the net, we can roughly see how good the net is. The loss is caluculated using the log-loss function and gives a numerical estimate of how good a classifer is. The lower the loss the better. The network has a loss of less than 0.001 which is really good, but this could mean that the network is overfitting. To check this we could use a validation set or we can try the network on the test set to see how well it predicts the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mlp.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predictions that our netowrk made, we can check them using a confusion matrix. A confusion matrix is a way to check how the guesses made differ from the truth values. It works by creating a CxC matrix where C is the number of categories where the diagonals of the matrix show true postivies guesses, values along a row that are not on the diagonals show how many false postivies for that class were made and values along a column that are not on the diagonals show how many false negitives were made.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[537,   0],\n",
       "       [  0, 496]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion = confusion_matrix(pred, test_Y, labels=np.unique(test_Y))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, we can see that our network can perfectly guess the difference between sitting and standing as only the diagonals are nonzero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on One Subject and Testing on Another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that our network can dicern between two different activities, let us try it again on just one subject to see how well it does with getting the differences between all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training and testing set once more\n",
    "sub1_train = df_train[df_train['subject'] == 1].values\n",
    "sub2_test = df_test[df_test['subject'] == 2].values\n",
    "\n",
    "sub1_train_X = sub1_train[:,:-2]\n",
    "sub1_train_Y = sub1_train[:,-1]\n",
    "sub1_train_X, sub1_train_Y = shuffle(sub1_train_X, sub1_train_Y)\n",
    "\n",
    "sub2_test_X = sub2_test[:,:-2]\n",
    "sub2_test_Y = sub2_test[:,-1]\n",
    "sub2_test_X, sub2_test_Y = shuffle(sub2_test_X, sub2_test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.MLPClassifier(hidden_layer_sizes=(100,50)).fit(sub1_train_X, sub1_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002776433545265388"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss here is around 0.003 which is still really good, but again this could be an over fitting issue so lets again use the confusion matrix to look at the quality of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48,  0,  0,  0,  0,  0],\n",
       "       [ 0, 16,  0,  0,  0,  0],\n",
       "       [ 0, 30, 54,  0,  0,  0],\n",
       "       [ 0,  0,  0, 54,  2,  0],\n",
       "       [ 0,  0,  0,  5, 45, 40],\n",
       "       [ 0,  0,  0,  0,  0,  8]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mlp.predict(sub2_test_X)\n",
    "\n",
    "confusion = confusion_matrix(pred, sub2_test_Y, labels=np.unique(sub2_test_Y))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
       "       'WALKING_UPSTAIRS'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sub2_test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the network right now is not as good at this task as it was before. The confusion matrix shows that there are some errors in the network's predictions as there are nonzero values outside of the diagonal of the matrix. So here we will try and provide a metric to decide roughly how good our netowrk is. We want to see the accuracy, percision, recall, and (BCR) of our network. Each one of these calcualtions are some combination of true positives (tp), false positives (fp), true negitives (tn), and false negitives (fn). Accuracy is (tp + tn) / (tp + tn + fn + fp), percision is tp / (tp + fp), recall is tp / (tp + fn), and BCR is (percision + recall) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model based on accuracy, percision, recall, and BCR\n",
    "acc_list, percision_list, recall_list, BCR_list = calc_metrics(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9150110375275938\n",
      "0.8511904761904763\n",
      "0.731198966570327\n",
      "0.7911947213804016\n"
     ]
    }
   ],
   "source": [
    "# Average accuracy\n",
    "print (np.sum(acc_list)/len(acc_list))\n",
    "\n",
    "# Average percision\n",
    "print (np.sum(percision_list)/len(percision_list))\n",
    "\n",
    "# Average recall\n",
    "print (np.sum(recall_list)/len(recall_list))\n",
    "\n",
    "# Average accuracy\n",
    "print (np.sum(BCR_list)/len(BCR_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average accuracy of the network is greater than 90%, but that does not tell the whole story. In the accuracy calculation a true negitive is when the network does not guess a particular class when it shouldn't. The problem here is that if we want to know the accuracy of class 1 any time the network is given a sample from outside class 1, no matter what guess the network makes so long as the guess is not class 1, it is counted as a tn. BCR is a more accurate calculation of how well the network is doing since it only looks at tp, fp, and fn. This allows is to penalize wrong guesses when it pertains to the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets look at those metrics per class to see what classes the network is having difficulties with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for LAYING\n",
      "  Accuracy: 1.000000\n",
      "  Percision: 1.000000\n",
      "  Recall: 1.000000\n",
      "  BCR: 1.000000\n",
      "Metrics for SITTING\n",
      "  Accuracy: 0.900662\n",
      "  Percision: 1.000000\n",
      "  Recall: 0.347826\n",
      "  BCR: 0.673913\n",
      "Metrics for STANDING\n",
      "  Accuracy: 0.900662\n",
      "  Percision: 0.642857\n",
      "  Recall: 1.000000\n",
      "  BCR: 0.821429\n",
      "Metrics for WALKING\n",
      "  Accuracy: 0.976821\n",
      "  Percision: 0.964286\n",
      "  Recall: 0.915254\n",
      "  BCR: 0.939770\n",
      "Metrics for WALKING_DOWNSTAIRS\n",
      "  Accuracy: 0.844371\n",
      "  Percision: 0.500000\n",
      "  Recall: 0.957447\n",
      "  BCR: 0.728723\n",
      "Metrics for WALKING_UPSTAIRS\n",
      "  Accuracy: 0.867550\n",
      "  Percision: 1.000000\n",
      "  Recall: 0.166667\n",
      "  BCR: 0.583333\n"
     ]
    }
   ],
   "source": [
    "act_list = np.unique(sub2_test_Y)\n",
    "for i in range(len(act_list)):\n",
    "    print(\"Metrics for %s\" % act_list[i])\n",
    "    print(\"  Accuracy: %f\" % acc_list[i])\n",
    "    print(\"  Percision: %f\" % percision_list[i])\n",
    "    print(\"  Recall: %f\" % recall_list[i])\n",
    "    print(\"  BCR: %f\" % BCR_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is really good at getting laying as it will never make a mistake with laying (BCR = 100%). However, the network struggles with sitting and walking upstairs, both having a BCR of less than 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Structure of the Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see there are some problems with our network, let's see if we can improve our network by chaning things like the activation function, number of layers, and number of neurons in each layer (learning rate, maybe try doing in pytorch as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training and testing set once more\n",
    "sub1_train = df_train[df_train['subject'] == 1].values\n",
    "sub2_test = df_test[df_test['subject'] == 2].values\n",
    "\n",
    "sub1_train_X = sub1_train[:,:-2]\n",
    "sub1_train_Y = sub1_train[:,-1]\n",
    "sub1_train_X, sub1_train_Y = shuffle(sub1_train_X, sub1_train_Y)\n",
    "\n",
    "sub2_test_X = sub2_test[:,:-2]\n",
    "sub2_test_Y = sub2_test[:,-1]\n",
    "sub2_test_X, sub2_test_Y = shuffle(sub2_test_X, sub2_test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our old network used ReLU as its activation function so let's try out two other activation functions: tanh and sigmoid. (do k-fold validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tanh = nn.MLPClassifier(hidden_layer_sizes=(100,50), activation=\"tanh\")\n",
    "mlp_sig = nn.MLPClassifier(hidden_layer_sizes=(100,50), activation='logistic', max_iter=500)\n",
    "mlp_ReLU = mlp = nn.MLPClassifier(hidden_layer_sizes=(100,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid network needed more iterations in order to converge, since we use early stopping for the other network this does not change the performance of the other networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the networks we will use K-Fold validation to expose each network to the full set of training data. KFold validation is a way for us to compare the quality of the networks by spliting the dataset into k different groups and training each network, leaving one of the groups out of training to use as a validation set, then testing the quality of the network on the validation set and repeating until every group has been used once. We will split the training data into 3 stratified sets and hold one set out to use as a validation set. This means that we seperate the data in a way that tries to keep the proportions of each class in the data set constant per set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'WALKING_UPSTAIRS' 'SITTING' 'STANDING'\n",
      " 'STANDING' 'WALKING_UPSTAIRS' 'SITTING' 'LAYING' 'WALKING' 'STANDING'\n",
      " 'STANDING' 'WALKING' 'STANDING' 'LAYING' 'WALKING' 'WALKING' 'STANDING'\n",
      " 'WALKING' 'STANDING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'LAYING' 'WALKING_UPSTAIRS' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'STANDING' 'WALKING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'STANDING' 'WALKING_UPSTAIRS' 'WALKING' 'SITTING'\n",
      " 'SITTING' 'LAYING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'LAYING'\n",
      " 'WALKING' 'LAYING' 'STANDING' 'WALKING_UPSTAIRS' 'WALKING' 'WALKING'\n",
      " 'LAYING' 'LAYING' 'WALKING_DOWNSTAIRS' 'STANDING' 'WALKING_DOWNSTAIRS'\n",
      " 'LAYING' 'SITTING' 'WALKING' 'WALKING_DOWNSTAIRS' 'STANDING'\n",
      " 'WALKING_DOWNSTAIRS' 'STANDING' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING' 'SITTING' 'WALKING_UPSTAIRS' 'WALKING' 'STANDING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'WALKING' 'LAYING' 'SITTING'\n",
      " 'LAYING' 'LAYING' 'WALKING' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'SITTING' 'WALKING' 'WALKING' 'LAYING' 'SITTING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING' 'WALKING_DOWNSTAIRS' 'SITTING' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'SITTING' 'LAYING' 'WALKING'\n",
      " 'STANDING' 'WALKING' 'WALKING' 'SITTING' 'LAYING' 'SITTING'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'SITTING' 'WALKING_DOWNSTAIRS' 'LAYING'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'WALKING' 'WALKING_DOWNSTAIRS' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS']\n",
      "['WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'WALKING_UPSTAIRS' 'SITTING' 'STANDING'\n",
      " 'STANDING' 'WALKING_UPSTAIRS' 'SITTING' 'LAYING' 'WALKING' 'STANDING'\n",
      " 'STANDING' 'WALKING' 'STANDING' 'LAYING' 'WALKING' 'WALKING' 'STANDING'\n",
      " 'WALKING' 'STANDING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'LAYING' 'WALKING_UPSTAIRS' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'STANDING' 'WALKING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'STANDING' 'WALKING_UPSTAIRS' 'WALKING' 'SITTING'\n",
      " 'SITTING' 'LAYING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'LAYING'\n",
      " 'WALKING' 'LAYING' 'STANDING' 'WALKING_UPSTAIRS' 'WALKING' 'WALKING'\n",
      " 'LAYING' 'LAYING' 'WALKING_DOWNSTAIRS' 'STANDING' 'WALKING_DOWNSTAIRS'\n",
      " 'LAYING' 'SITTING' 'WALKING' 'WALKING_DOWNSTAIRS' 'STANDING'\n",
      " 'WALKING_DOWNSTAIRS' 'STANDING' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING' 'SITTING' 'WALKING_UPSTAIRS' 'WALKING' 'STANDING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'WALKING' 'LAYING' 'SITTING'\n",
      " 'LAYING' 'LAYING' 'WALKING' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'SITTING' 'WALKING' 'WALKING' 'LAYING' 'SITTING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING' 'WALKING_DOWNSTAIRS' 'SITTING' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'SITTING' 'LAYING' 'WALKING'\n",
      " 'STANDING' 'WALKING' 'WALKING' 'SITTING' 'LAYING' 'SITTING'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'SITTING' 'WALKING_DOWNSTAIRS' 'LAYING'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'WALKING' 'WALKING_DOWNSTAIRS' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS']\n",
      "['STANDING' 'STANDING' 'WALKING_UPSTAIRS' 'STANDING' 'SITTING' 'SITTING'\n",
      " 'WALKING' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'WALKING_UPSTAIRS'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING_UPSTAIRS' 'SITTING' 'WALKING_UPSTAIRS' 'WALKING' 'LAYING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'LAYING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'STANDING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING' 'WALKING' 'LAYING' 'WALKING_DOWNSTAIRS'\n",
      " 'LAYING' 'LAYING' 'SITTING' 'WALKING_UPSTAIRS' 'SITTING'\n",
      " 'WALKING_DOWNSTAIRS' 'LAYING' 'SITTING' 'LAYING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'LAYING' 'STANDING' 'WALKING' 'WALKING' 'WALKING'\n",
      " 'WALKING' 'WALKING' 'LAYING' 'STANDING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'WALKING' 'WALKING' 'WALKING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING'\n",
      " 'SITTING' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'LAYING' 'WALKING' 'LAYING' 'STANDING' 'STANDING'\n",
      " 'STANDING' 'STANDING' 'WALKING' 'LAYING' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'SITTING' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'WALKING' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'WALKING' 'SITTING'\n",
      " 'STANDING' 'WALKING_UPSTAIRS' 'LAYING' 'SITTING' 'STANDING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS' 'SITTING' 'SITTING' 'SITTING'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'SITTING' 'STANDING' 'WALKING' 'STANDING'\n",
      " 'WALKING' 'WALKING' 'STANDING' 'STANDING']\n",
      "['STANDING' 'STANDING' 'WALKING_UPSTAIRS' 'STANDING' 'SITTING' 'SITTING'\n",
      " 'WALKING' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'WALKING_UPSTAIRS'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING_UPSTAIRS' 'SITTING' 'WALKING_UPSTAIRS' 'WALKING' 'LAYING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'LAYING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'STANDING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING' 'WALKING' 'LAYING' 'WALKING_DOWNSTAIRS'\n",
      " 'LAYING' 'LAYING' 'SITTING' 'WALKING_UPSTAIRS' 'SITTING'\n",
      " 'WALKING_DOWNSTAIRS' 'LAYING' 'SITTING' 'LAYING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'LAYING' 'STANDING' 'WALKING' 'WALKING' 'WALKING'\n",
      " 'WALKING' 'WALKING' 'LAYING' 'STANDING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'WALKING' 'WALKING' 'WALKING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING'\n",
      " 'SITTING' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'LAYING' 'WALKING' 'LAYING' 'STANDING' 'STANDING'\n",
      " 'STANDING' 'STANDING' 'WALKING' 'LAYING' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'SITTING' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'WALKING' 'WALKING_DOWNSTAIRS' 'LAYING' 'WALKING' 'WALKING' 'SITTING'\n",
      " 'STANDING' 'WALKING_UPSTAIRS' 'LAYING' 'SITTING' 'STANDING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS' 'SITTING' 'SITTING' 'SITTING'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'SITTING' 'STANDING' 'WALKING' 'STANDING'\n",
      " 'WALKING' 'WALKING' 'STANDING' 'STANDING']\n",
      "['LAYING' 'LAYING' 'LAYING' 'WALKING_DOWNSTAIRS' 'LAYING' 'LAYING'\n",
      " 'LAYING' 'LAYING' 'LAYING' 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'SITTING' 'LAYING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'LAYING' 'WALKING_UPSTAIRS'\n",
      " 'SITTING' 'WALKING_DOWNSTAIRS' 'WALKING' 'WALKING_DOWNSTAIRS' 'SITTING'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'WALKING' 'WALKING' 'SITTING' 'WALKING'\n",
      " 'STANDING' 'WALKING' 'WALKING' 'STANDING' 'WALKING_UPSTAIRS' 'LAYING'\n",
      " 'WALKING' 'STANDING' 'LAYING' 'WALKING' 'WALKING' 'WALKING_UPSTAIRS'\n",
      " 'LAYING' 'WALKING_UPSTAIRS' 'STANDING' 'WALKING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'STANDING' 'SITTING' 'SITTING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'SITTING' 'WALKING' 'STANDING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'STANDING' 'WALKING_DOWNSTAIRS' 'LAYING' 'STANDING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'SITTING' 'STANDING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING' 'LAYING' 'SITTING' 'SITTING' 'SITTING'\n",
      " 'STANDING' 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING' 'STANDING' 'STANDING' 'WALKING' 'STANDING' 'WALKING_UPSTAIRS'\n",
      " 'STANDING' 'WALKING' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'WALKING' 'LAYING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'WALKING' 'STANDING' 'WALKING'\n",
      " 'WALKING' 'STANDING']\n",
      "['LAYING' 'LAYING' 'LAYING' 'WALKING_DOWNSTAIRS' 'LAYING' 'LAYING'\n",
      " 'LAYING' 'LAYING' 'LAYING' 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'SITTING' 'LAYING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'LAYING' 'WALKING_UPSTAIRS'\n",
      " 'SITTING' 'WALKING_DOWNSTAIRS' 'WALKING' 'WALKING_DOWNSTAIRS' 'SITTING'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING_UPSTAIRS' 'WALKING' 'WALKING' 'WALKING' 'SITTING' 'WALKING'\n",
      " 'STANDING' 'WALKING' 'WALKING' 'STANDING' 'WALKING_UPSTAIRS' 'LAYING'\n",
      " 'WALKING' 'STANDING' 'LAYING' 'WALKING' 'WALKING' 'WALKING_UPSTAIRS'\n",
      " 'LAYING' 'WALKING_UPSTAIRS' 'STANDING' 'WALKING' 'SITTING'\n",
      " 'WALKING_UPSTAIRS' 'STANDING' 'SITTING' 'SITTING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'SITTING' 'SITTING' 'WALKING' 'STANDING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS' 'STANDING'\n",
      " 'STANDING' 'WALKING_DOWNSTAIRS' 'LAYING' 'STANDING' 'WALKING'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS' 'SITTING' 'STANDING' 'WALKING'\n",
      " 'WALKING_DOWNSTAIRS' 'WALKING' 'LAYING' 'SITTING' 'SITTING' 'SITTING'\n",
      " 'STANDING' 'WALKING_DOWNSTAIRS' 'WALKING_DOWNSTAIRS' 'WALKING_UPSTAIRS'\n",
      " 'WALKING' 'STANDING' 'STANDING' 'WALKING' 'STANDING' 'WALKING_UPSTAIRS'\n",
      " 'STANDING' 'WALKING' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING' 'WALKING' 'WALKING' 'WALKING' 'LAYING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_UPSTAIRS' 'WALKING_UPSTAIRS' 'WALKING' 'STANDING' 'WALKING'\n",
      " 'WALKING' 'STANDING']\n"
     ]
    }
   ],
   "source": [
    "# Create lists to store the results of each fold for each network\n",
    "metrics_tanh = []\n",
    "metrics_sig = []\n",
    "metrics_ReLU = []\n",
    "\n",
    "# Create the k splits\n",
    "kf = StratifiedKFold(n_splits=3)\n",
    "for train_idx, val_idx in kf.split(sub1_train_X, sub1_train_Y):\n",
    "    # For each split seperate the training and the validation set\n",
    "    train_X, val_X = sub1_train_X[train_idx], sub1_train_X[val_idx]\n",
    "    train_Y, val_Y = sub1_train_Y[train_idx], sub1_train_Y[val_idx]\n",
    "    \n",
    "    # Train each network on the split\n",
    "    mlp_tanh_train = mlp_tanh.fit(train_X, train_Y)\n",
    "    mlp_sig_train = mlp_tanh.fit(train_X, train_Y)\n",
    "    mlp_ReLU_train = mlp_tanh.fit(train_X, train_Y)\n",
    "    \n",
    "    # Evaluate each network\n",
    "    pred_tanh = mlp_tanh_train.predict(val_X)\n",
    "    confusion_tanh = confusion_matrix(pred_tanh, val_Y, labels=np.unique(val_Y))\n",
    "    metrics_tanh.append(calc_metrics(confusion_tanh))\n",
    "\n",
    "    pred_sig = mlp_sig_train.predict(val_X)\n",
    "    confusion_sig = confusion_matrix(pred_sig, val_Y, labels=np.unique(val_Y))\n",
    "    metrics_sig.append(calc_metrics(confusion_sig))\n",
    "    \n",
    "    pred_ReLU = mlp_ReLU_train.predict(val_X)\n",
    "    confusion_ReLU = confusion_matrix(pred_ReLU, val_Y, labels=np.unique(val_Y))\n",
    "    metrics_ReLU.append(calc_metrics(confusion_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 1.5, 1.5, 1.5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(metics_tanh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_tanh[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the BCR for each activation function it seems that tanh is better than the rest with a BCR of 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger and smaller networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets change around the netowrk size to see if a larger network or a smaller network would work better. Since we know that tanh is the better activation function, we will use that in these networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_onelayer = nn.MLPClassifier(hidden_layer_sizes=(100), activation=\"tanh\").fit(sub1_train_X, sub1_train_Y)\n",
    "mlp_less_neurons = nn.MLPClassifier(hidden_layer_sizes=(50, 25), activation=\"tanh\").fit(sub1_train_X, sub1_train_Y)\n",
    "mlp_more_neurons = nn.MLPClassifier(hidden_layer_sizes=(200, 100), activation=\"tanh\").fit(sub1_train_X, sub1_train_Y)\n",
    "mlp_threelayer = nn.MLPClassifier(hidden_layer_sizes=(100, 50, 50), activation=\"tanh\").fit(sub1_train_X, sub1_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for tanh activation function:\n",
      "0.9205298013245033\n",
      "0.8495155150981709\n",
      "0.7454774385856718\n",
      "0.7974964768419214\n"
     ]
    }
   ],
   "source": [
    "pred_onelayer = mlp_onelayer.predict(sub2_test_X)\n",
    "confusion_onelayer = confusion_matrix(pred_onelayer, sub2_test_Y, labels=np.unique(sub2_test_Y))\n",
    "metrics_onelayer = calc_metrics(confusion_onelayer)\n",
    "print(\"Metrics for tanh activation function:\")\n",
    "for metric in metrics_onelayer:\n",
    "    print (np.sum(metric)/len(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for tanh activation function:\n",
      "0.9172185430463577\n",
      "0.8415352763178849\n",
      "0.7347588395518553\n",
      "0.7881470579348702\n"
     ]
    }
   ],
   "source": [
    "pred_less_neurons = mlp_less_neurons.predict(sub2_test_X)\n",
    "confusion_less_neurons = confusion_matrix(pred_less_neurons, sub2_test_Y, labels=np.unique(sub2_test_Y))\n",
    "metrics_less_neurons = calc_metrics(confusion_less_neurons)\n",
    "print(\"Metrics for tanh activation function:\")\n",
    "for metric in metrics_less_neurons:\n",
    "    print (np.sum(metric)/len(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for tanh activation function:\n",
      "0.9238410596026491\n",
      "0.8569775132275131\n",
      "0.7557399270223044\n",
      "0.8063587201249088\n"
     ]
    }
   ],
   "source": [
    "pred_more_neurons = mlp_more_neurons.predict(sub2_test_X)\n",
    "confusion_more_neurons = confusion_matrix(pred_more_neurons, sub2_test_Y, labels=np.unique(sub2_test_Y))\n",
    "metrics_more_neurons = calc_metrics(confusion_more_neurons)\n",
    "print(\"Metrics for tanh activation function:\")\n",
    "for metric in metrics_more_neurons:\n",
    "    print (np.sum(metric)/len(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for tanh activation function:\n",
      "0.9183222958057394\n",
      "0.8435637891520243\n",
      "0.7380800955905027\n",
      "0.7908219423712636\n"
     ]
    }
   ],
   "source": [
    "pred_threelayer = mlp_threelayer.predict(sub2_test_X)\n",
    "confusion_threelayer = confusion_matrix(pred_threelayer, sub2_test_Y, labels=np.unique(sub2_test_Y))\n",
    "metrics_threelayer = calc_metrics(confusion_threelayer)\n",
    "print(\"Metrics for tanh activation function:\")\n",
    "for metric in metrics_threelayer:\n",
    "    print (np.sum(metric)/len(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the BCR for each of the trials to the one obtained from the tanh model, we can see that increasing the number of layers imporved the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.values\n",
    "test = df_test.values\n",
    "\n",
    "train_X = train[:,:-2]\n",
    "train_Y = train[:,-1]\n",
    "train_X, train_Y = shuffle(train_X, train_Y)\n",
    "\n",
    "test_X = test[:,:-2]\n",
    "test_Y = test[:,-1]\n",
    "test_X, test_Y = shuffle(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_makeup = (100,100,100,50,50)\n",
    "mlp = nn.MLPClassifier(hidden_layer_sizes=layer_makeup, activation=\"tanh\").fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015939948597319648"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a training loss of about 0.03. While this is higher than before, it is still fine. This could show that the network has stopped overfitting and is performing even better than before. We will check this by checking its results on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[537,   0,   0,   0,   0,   0],\n",
       "       [  0, 458,  39,   0,   0,   0],\n",
       "       [  0,  31, 492,   0,   0,   0],\n",
       "       [  0,   0,   1, 489,   4,  27],\n",
       "       [  0,   0,   0,   4, 400,  14],\n",
       "       [  0,   2,   0,   3,  16, 430]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mlp.predict(test_X)\n",
    "confusion = confusion_matrix(pred, test_Y, labels=np.unique(test_Y))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9840515778758059\n",
      "0.9518683355228538\n",
      "0.9514702451651931\n",
      "0.9516692903440234\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model based on accuracy, percision, recall, and BCR\n",
    "acc_list, percision_list, recall_list, BCR_list = calc_metrics(confusion)\n",
    "\n",
    "# Average accuracy\n",
    "print (np.sum(acc_list)/len(acc_list))\n",
    "\n",
    "# Average percision\n",
    "print (np.sum(percision_list)/len(percision_list))\n",
    "\n",
    "# Average recall\n",
    "print (np.sum(recall_list)/len(recall_list))\n",
    "\n",
    "# Average accuracy\n",
    "print (np.sum(BCR_list)/len(BCR_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for LAYING\n",
      "  Accuracy: 1.000000\n",
      "  Percision: 1.000000\n",
      "  Recall: 1.000000\n",
      "  BCR: 1.000000\n",
      "Metrics for SITTING\n",
      "  Accuracy: 0.975568\n",
      "  Percision: 0.921529\n",
      "  Recall: 0.932790\n",
      "  BCR: 0.927160\n",
      "Metrics for STANDING\n",
      "  Accuracy: 0.975908\n",
      "  Percision: 0.940727\n",
      "  Recall: 0.924812\n",
      "  BCR: 0.932769\n",
      "Metrics for WALKING\n",
      "  Accuracy: 0.986766\n",
      "  Percision: 0.938580\n",
      "  Recall: 0.985887\n",
      "  BCR: 0.962233\n",
      "Metrics for WALKING_DOWNSTAIRS\n",
      "  Accuracy: 0.987106\n",
      "  Percision: 0.956938\n",
      "  Recall: 0.952381\n",
      "  BCR: 0.954659\n",
      "Metrics for WALKING_UPSTAIRS\n",
      "  Accuracy: 0.978962\n",
      "  Percision: 0.953437\n",
      "  Recall: 0.912951\n",
      "  BCR: 0.933194\n"
     ]
    }
   ],
   "source": [
    "act_list = np.unique(test_Y)\n",
    "for i in range(len(act_list)):\n",
    "    print(\"Metrics for %s\" % act_list[i])\n",
    "    print(\"  Accuracy: %f\" % acc_list[i])\n",
    "    print(\"  Percision: %f\" % percision_list[i])\n",
    "    print(\"  Recall: %f\" % recall_list[i])\n",
    "    print(\"  BCR: %f\" % BCR_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average BCR here has really imporved. It is at the point where the network can correctly guess what actiivity someone is doing about 95% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
